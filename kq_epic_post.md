## _Super cool title that I’ll figure out later_

Don’t forget the tl:dr here

### The Quest
Over in the Pull Requests team, we have been undertaking a noble effort to eliminate high-event killed queries. If you take a peek at our [scorecard](https://catalog.githubapp.com/services/github/pull_requests/scorecards/mysql-database-usage), you'll see that there is an expectation of "_no killed queries on [primary/replica] database hosts generated by this service_". Supporting the full Pull Requests experience and all that it entails means a lot of queries across web and API calls. 

If one of those takes longer than 10 seconds, it is killed and the data is not returned. This might happen if a lot of data is being requested, the query itself is inefficient, there is downtime in another service, or a variety of other things. With availability as a priority, we began an epic to fix the problematic queries and get our scorecard to green.

DATA HERE? 
- Number of endpoints in service
- Total requests to all service endpoints & number of queries made by each
- Side by side comparison of our query count vs. other high traffic pages
- Total queries by endpoint vs. how many of those are killed (before and after)
https://app.datadoghq.com/dashboard/y5d-jze-999/list-of-endpoints-owned-by-pull-requests?fromUser=false&fullscreen_end_ts=1742917134905&fullscreen_paused=false&fullscreen_refresh_mode=sliding&fullscreen_section=overview&fullscreen_start_ts=1742830734905&fullscreen_widget=8204614155363614&refresh_mode=sliding&from_ts=1742823555403&to_ts=1742909955403&live=true
- Splunk to get total killed count before & after
https://github.com/github/mysql-database-usage/blob/696444dd24eec9358e3539e60ad4a5ac38102640/fetch_splunk_data.rb
- % of killed queries (timed out, other wording to make it clearer?) out of the total number queries before/after
- And contextualize that the improvement improved customer experience
- Point out that the same queries run without timing out after
- List of common query killers?

### Gather your equipment
The first step in any valiant quest is to study your map. So, we started with the basics
- What are the queries we need to solve? 
- Where are they originating?
- How many times are they happening per week?
- How long do they take on average? 

This information was available in the automated issues attributed to our service in the [mysql-database-usage repo](https://github.com/github/mysql-database-usage/issues?q=is%3Aissue%20state%3Aopen%20github%2Fpull_requests%20). With the data laid out, we could focus and prioritize. So just work your way from the top, right? Well, we soon found there were a few snags to this seemingly simple approach.

1. **The scorecard isn't static**. Just because we started with a set of queries doesn't mean that we'd have that same set for the entirety of the project. An existing query might be improved by an indirect change, or a new problematic query could arise from updated code or processes. This makes setting a goal difficult as it's not just working through a backlog but also making sure to keep an eye on what is exiting and joining the group.
2. **Query identifiers change**. Each query is attached to a `fingerprint` as a unique identifier. For the most part, this is a good way to track performance over time and find it in logs. But, sometimes a query's fingerprint changes. For example, in late November, several of our affected queries appeared to have been solved, but at the same time new issues were being created with identical query structures and entry points but new fingerprints, [as explained here](https://github.com/github/mysql-database-scorecard/issues/57). This meant that we had to be vigilant about cross-referencing old query issues to new ones.
3. **Benchmarks change**. Killed query issues are regularly updated via automation with current performance data. This was one of the benchmarks used to determine impact and set priorities. In November, we noticed that we weren't seeing updates from the bot so we couldn't be sure that our data was accurate or current. Eventually a fix was made in January so issues were updating again, but in the interim, we couldn't easily determine how to find that information manually.

### Plan your adventure
With those challenges in mind, we began to strategize. We started by grouping our initial set of queries by possible solutions, including [batching](https://github.com/github/pull-requests/issues/14462) and [adding new indexes](https://github.com/github/pull-requests/issues/14461). However, there’s one bucket that stands out as the most fraught and the most impactful: our dependency on the `issues.state` field.

A common trait among these killed queries was a non-performant join on the `issues` table. For reasons lost to time, pull requests still depended on issues to determine their state. This resulted in queries that looked something like this:
```
SELECT
    COUNT (*)
  FROM
    `pull_requests`
    , `issues`
  WHERE
    `pull_requests`.`repository_id` = ?
    AND (
      `pull_requests`.`user_hidden` = ?
      OR `pull_requests`.`user_id` = ?
    )
    AND `pull_requests`.`merged_at` IS NULL
    AND `issues`.`state` = ?
    AND `issues`.`pull_request_id` = `pull_requests`.`id`
    AND `issues`.`repository_id` = `pull_requests`.`repository_id` LIMIT ?
```
The most basic question here is "Why can't a pull request have its own state?". The answer seemed like a clear win. Put a `state` column on `pull_requests` and remove the expensive join on issues `state`. Easy peasy.

### Face the challengers
Spoiler alert: It was not easy peasy. It was not even in the same realm as easy peasy. Like a labyrinth, the more turns we took, the more twists we discovered.

#### Obstacle 1: When is a state not a state
An issue can be one of two states - open or closed. The same is true for pull requests at the database level. However at the model level, a pull request can have three states: open, closed, or merged, as determined by the [PullRequest#state method](https://github.com/github/github/blob/a18286f131f4dc80dcbab605938125fcd9ac509d/packages/pull_requests/app/models/pull_request.rb#L650-L654). The existence of this method blocked us from simply adding a `pull_requests.state` field. We briefly explored some workarounds for this naming conflict, however, the best  solution ended up being the simplest one; choose a different column name. With `pull_requests.status` in place, we could continue on our way.

#### Obstacle 2: We've had one, yes, but what about second join
While the majority of killed queries had a single join to issues for the state, there were some that were a bit more greedy[^1]. These were GraphQL calls looking for a list of pull requests by label, and they required multiple joins to form a query that looked something like this:

```
SELECT
    `pull_requests`.`id`
  FROM
    `pull_requests`
    , `issues`
    , `issues_labels`
    , `labels`
  WHERE
    `labels`.`lowercase_name` IN (
      . . .
    )
    AND `pull_requests`.`repository_id` = ?
    AND (
      `pull_requests`.`user_hidden` = ?
      OR `pull_requests`.`user_id` = ?
    )
    AND `issues`.`state` = ?
    AND `issues`.`pull_request_id` = `pull_requests`.`id`
    AND `issues`.`repository_id` = `pull_requests`.`repository_id`
    AND `issues_labels`.`issue_id` = `issues`.`id`
    AND `labels`.`id` = `issues_labels`.`label_id`
    AND `labels`.`repository_id` = `issues`.`repository_id`
  ORDER BY
    `pull_requests`.`id` DESC LIMIT ?
```

Simply swapping `issues.state` for `pull_request.status` wasn't enough for these queries; they were still timing out. Thus we explored other optimizations, including breaking out subqueries, forcing or ignoring an index, and reorganizing the query structure, none of which saw  improvements. We even considered storing label ids in a new column on `issues` to eliminate the extra joins, but that presented its own very big lift that seemed far outside the scope of this epic.

So, like the dwarves in the mines of Moria, we dug deeper to determine _who_ was hitting these queries. Over a period of 30 days, we found that between the two queries, all calls were coming from only 4 users within 4 repos. More interestingly, all 4 users were GitHub apps mimicking merge queue functionality. One in particular was causing around 10k events routinely every 20 minutes. With this new information, our minds turned towards rate limiting. Unfortunately, rate limits are set based on individual app installations and cannot be applied by query fingerprint, so they were too blunt to deploy here.

Once we'd exhausted our best efforts, we ultimately decided to forgo this particular quest lest we awaken the Balrog. Due to the very small group of affected users, we determined that the estimated effort to continue was not worth the potential impact. Instead, there is an open discussion about updating our scorecard tooling to exempt these queries from our scorecards moving forward.

#### Obstacle 3: The road goes ever on (or some sort of more descriptive, yet fanciful, title)
To populate our new `pull_requests.status`column, we backfilled it with the data from issues.state using a transition. `pull_requests` is a _big_ table, so we knew we were in for a hefty process. What we didn't expect is that it would take 604 hours of run time across a period of 33 days, including dry runs and write runs. Within that, we had to maintain vigilance as we encountered unexplained failures that would interrupt the process, requiring us to manually restart. Finally, when we thought our toil had borne its reward, we were faced with some confusing data.

While the logging indicated everything had run as expected, Kusto, datadot, and analytics hosts suggested the backfill had missed some pull requests[^2]. This was potentially an enormous setback. To find a reliable source of truth, we reached out to several teams and finally learned that what we needed was a new full snapshot of the table. With that done, we were able to determine that the backfill was mostly successful and the remaining group of pull requests missing `status` data were from the period after dual writing was introduced.

#### Obstacle 4:  
With that knowledge, we started round two. First, we fixed our dual writing bug

PUT WHAT HAPPENED AND WHAT WE DID HERE.
Include info about the gap between old/new dual writing and how we had to run a targeted transition/background job/etc


### Become the heroes

[Scientist](https://github.com/github/scientist) was our champion in this epic as we ran 12 separate experiments to validate our changes.

### Revel in your triumph

Though the quest was arduous and many shadows befell us, at last we reaped the harvest of victory. (i hope lol)

Results with ✨graphs ✨

Overall results

### Reflect on your journey

why are killed query scorecards all or nothing
why is it so hard to query for seemingly simple data
how can we better assure and confirm that our data
how can we improve transitions

Learned - don’t remove old code at the same time as adding new code if they do the same things. Start a public channel early. Communicate with other teams early and make your intentions known if it crosses with another team.


[^1]: If you want to see the full journey, you can follow along [here](https://github.com/github/mysql-database-usage/issues/1473#issuecomment-2539459465).
[^2]: https://github.com/github/pull-requests/issues/15675#issuecomment-2640038704
