## Super cool title that I‚Äôll figure out later

Copilot's suggestions 
(_for some reason CP insists that a title must contain a colon ü§∑_)
- Fixing the Hitch in Our Database Giddy-Up _(looolllll ü§†)_
- Debugging the Database Blues
- Optimizing Pull Request Queries: The Journey to a Greener MySQL Scorecard
- Slaying the Query Dragon: The Quest for a Greener Scorecard _(i like this one, but, like, should it be more Lord of the Rings-y? üêâ)_
- The Fellowship of the Index: Battling Killed Queries in the Pull Request Realm
- Legends of Optimization: How We Conquered the Query Wasteland
- The Chronicles of MySQL: A Hero's Journey to Database Glory
- Vanquishing the Bottleneck Beast: A Saga of Query Redemption
- The Quest for the Golden Scorecard: A Tale of Indexes and Triumph
- From Shadows to Light: A Heroic Journey of Pull Request Queries _(whoa, intense)_
- The Database Chronicles: How We Defeated the Killed Query Curse
- Forging the Index Blade: A Database Adventure for the Ages

**Don‚Äôt forget the tl:dr here**

---
### The Quest
Over in the Pull Requests team, we have been undertaking a noble effort to eliminate high-event killed and slow queries. If you take a peek at our [scorecard](https://catalog.githubapp.com/services/github/pull_requests/scorecards/mysql-database-usage), you'll see that there is an expectation of "_no killed queries on [primary/replica] database hosts generated by this service_". Supporting the full Pull Requests experience and all that it entails means a lot of queries across web and API calls. 

Queries taking longer than 2 seconds are considered "slow" and those taking longer than 10 seconds are "killed". These might happen if a lot of data is being requested, the query itself is inefficient, there is downtime in another service, or a variety of other things. With availability as a priority, we began an epic to fix the problematic queries and get our scorecard to green.

_DATA HERE_

- _Number of endpoints in service_

- _Total requests to all service endpoints & number of queries made by each_

- _Side by side comparison of our query count vs. other high traffic pages_

- _[Total queries by endpoint](https://app.datadoghq.com/dashboard/y5d-jze-999/list-of-endpoints-owned-by-pull-requests?fromUser=false&fullscreen_end_ts=1742917134905&fullscreen_paused=false&fullscreen_refresh_mode=sliding&fullscreen_section=overview&fullscreen_start_ts=1742830734905&fullscreen_widget=8204614155363614&refresh_mode=sliding&from_ts=1742823555403&to_ts=1742909955403&live=true) vs. how many of those are killed (before and after)_

- _[Splunk to get total killed count before & after](https://github.com/github/mysql-database-usage/blob/696444dd24eec9358e3539e60ad4a5ac38102640/fetch_splunk_data.rb)_

- _% of killed queries (timed out, other wording to make it clearer?) out of the total number queries before/after_

- _Contextualize that the improvement improved customer experience_
  
- _Point out that the same queries run without timing out after_
  
- _List of common query killers?_

### Gather your equipment
The first step in any valiant quest is to study your map. So, we started with the basics:
- What are the queries we need to solve? 
- Where are they originating?
- How many times are they happening per week?
- How long do they take on average? 

This information was available in the automated issues attributed to our service in the [mysql-database-usage repo](https://github.com/github/mysql-database-usage/issues?q=is%3Aissue%20state%3Aopen%20github%2Fpull_requests%20). With the data laid out, we could focus and prioritize. So just work your way from the top, right? Well, we soon found there were a few snags to this seemingly simple approach.

1. **The scorecard isn't static**. Just because we started with a set of queries doesn't mean that we'd have that same set for the entirety of the project. An existing query might be improved by an indirect change, or a new problematic query could arise from updated code or processes. This makes setting a goal difficult as it's not just working through a backlog but also making sure to keep an eye on what is exiting and joining the group.
2. **Query identifiers change**. Each query is attached to a `fingerprint` as a unique identifier. For the most part, this is a good way to track performance over time and find queries in logs. But, sometimes a query's fingerprint changes. For example, in late November 2024, several of our affected queries appeared to have been solved, but at the same time new issues were being created with identical query structures and entry points but new fingerprints, [as explained here](https://github.com/github/mysql-database-scorecard/issues/57). This meant that we had to be vigilant about cross-referencing old query issues to new ones.
3. **Benchmarks change**. Killed query issues are regularly updated via automation with current performance data. This was one of the benchmarks used to determine impact and set priorities. Twice during this epic we noticed that we weren't seeing updates from the bot for several weeks. This meant that we couldn't be sure if our data was accurate or current. Eventually, fixes were made so that issues were updating again, but in the interim, we couldn't easily determine how to find that information manually.

### Plan your adventure
With those challenges in mind, we began to strategize. We started by grouping our initial set of queries by possible solutions, including [batching](https://github.com/github/pull-requests/issues/14462) and [adding new indexes](https://github.com/github/pull-requests/issues/14461). However, there‚Äôs one bucket that stands out as the most fraught and the most impactful: our dependency on the `issues.state` field.

A common trait among these killed queries was a non-performant join on the `issues` table. For reasons lost to time, pull requests still depended on issues to determine their state. This resulted in queries that looked something like this:
```
SELECT
    COUNT (*)
  FROM
    `pull_requests`
    , `issues`
  WHERE
    `pull_requests`.`repository_id` = ?
    AND (
      `pull_requests`.`user_hidden` = ?
      OR `pull_requests`.`user_id` = ?
    )
    AND `pull_requests`.`merged_at` IS NULL
    AND `issues`.`state` = ?
    AND `issues`.`pull_request_id` = `pull_requests`.`id`
    AND `issues`.`repository_id` = `pull_requests`.`repository_id` LIMIT ?
```
The most basic question here is "Why can't a pull request have its own state?". The answer seemed like a clear win. Put a `state` column on `pull_requests` and remove the expensive join on issues `state`. Easy peasy.

### Face the challengers
Spoiler alert: It was not easy peasy. It was not even in the same realm as easy peasy. Like a labyrinth, the more turns we took, the more twists we discovered.

#### Obstacle 1: When is a state not a state
An issue can be one of two states - open or closed. The same is true for pull requests at the database level. However at the model level, a pull request can have three states: open, closed, or merged, as determined by the [PullRequest#state method](https://github.com/github/github/blob/a18286f131f4dc80dcbab605938125fcd9ac509d/packages/pull_requests/app/models/pull_request.rb#L650-L654). The existence of this method blocked us from simply adding a `pull_requests.state` field. We briefly explored some workarounds for this naming conflict, however, the best  solution ended up being the simplest one; choose a different column name. With `pull_requests.status` in place, we could continue on our way.

#### Obstacle 2: We've had one, yes, but what about second join
While the majority of the queries had a single join to issues for the state, there were some that were a bit more greedy. These came from GraphQL calls looking for a list of pull requests by label, and they required multiple joins to form a query that looked something like this:

```
SELECT
    `pull_requests`.`id`
  FROM
    `pull_requests`
    , `issues`
    , `issues_labels`
    , `labels`
  WHERE
    `labels`.`lowercase_name` IN (
      . . .
    )
    AND `pull_requests`.`repository_id` = ?
    AND (
      `pull_requests`.`user_hidden` = ?
      OR `pull_requests`.`user_id` = ?
    )
    AND `issues`.`state` = ?
    AND `issues`.`pull_request_id` = `pull_requests`.`id`
    AND `issues`.`repository_id` = `pull_requests`.`repository_id`
    AND `issues_labels`.`issue_id` = `issues`.`id`
    AND `labels`.`id` = `issues_labels`.`label_id`
    AND `labels`.`repository_id` = `issues`.`repository_id`
  ORDER BY
    `pull_requests`.`id` DESC LIMIT ?
```

Simply swapping `issues.state` for `pull_request.status` wasn't enough for these queries; they were still timing out. Thus we explored other optimizations, including breaking out subqueries, forcing or ignoring an index, and reorganizing the query structure, none of which showed improvements[^1]. We even considered storing label ids in a new column on `issues` to eliminate the extra joins, but that presented its own big lift that seemed far outside the scope of this epic.

So, like the dwarves in the mines of Moria, we dug deeper to determine _who_ was hitting these queries. Over a period of 30 days, we found that between the two queries, all calls were coming from only 4 users within 4 repos. More interestingly, all 4 users were GitHub apps mimicking merge queue functionality. One in particular was causing around 10k events routinely every 20 minutes. With this new information, our minds turned towards rate limiting. Unfortunately, rate limits are set based on individual app installations and cannot be applied by query fingerprint, so they were too blunt to deploy here.

Once we'd exhausted our best efforts, we ultimately decided to forgo this particular quest lest we awaken the Balrog. Due to the very small group of affected users, we determined that the estimated effort to continue was not worth the potential impact. Instead, there is an open discussion about updating our scorecard tooling to exempt these queries from our scorecards moving forward.

#### Obstacle 3: The road goes ever on
To populate our new `pull_requests.status`column, we needed to backfill with the data from issues.state. `pull_requests` is a _big_ table, so we knew we were in for a hefty process. What we didn't expect is that the transition would take 604 hours of run time across a period of 33 days, including dry runs. Within that, we had to maintain vigilance as we encountered unexplained failures that would interrupt the process, requiring us to manually restart. Finally, when we thought our toil had borne its reward, we were faced with some confusing data.

While the logging indicated everything had run as expected, Kusto, datadot, and analytics hosts suggested the backfill had missed some pull requests[^2]. This was potentially an enormous setback. To find a reliable source of truth, we reached out to several teams and finally learned that what we needed was a new full snapshot of the table. With that done, we were able to determine that the backfill was mostly successful and the remaining group of pull requests missing `status` data were from the period after dual writing was introduced.

#### Obstacle 4: But they were all of them deceived, for another bug was made
And so we began fixing our dual writing bug, [a saga in itself](https://github.com/github/pull-requests/issues/15850). Our initial implementation set a pull request's status at the same time that an issue state was set via an existing ActiveRecord callback on the issue[^3]. The unexpected flaw with that was related to how an issue is created just before its associated pull request. Our first thought was to move this into its own callback on the pull request, but we decided to give orchestrations a try after some feedback from the Issues team[^4]. But we still encountered new mismatched records as there now a race condition between two orchestration processes and further iterations on this path caused deadlocks. Ultimately, the solution involved a callback on the issue that was localized to update pull request `status` whenever an issue's `state` changed. Phew.

And yet, the wraiths were still at our heels. Due to the multiple iterations of bug fixing, we had to run more transitions to update pull requests records. In one, we discovered that our use of Active Record lookups allowed for a gap between reading and writing and that direct queries were needed. In another, we learned how to be more targeted by creating a CSV of mismatched pull requests and iterating over them directly.

### Revel in your triumph

Though the quest was arduous and many shadows befell us, at last we reaped the harvest of victory. (_i hope :lolsob:_)

In this epic as we ran 12 [Scientist](https://github.com/github/scientist) experiments to validate our changes, some with multiple iterations.

_Results with ‚ú®graphs‚ú®_

### Reflect on your journey

This project spanned many months and thus there was a lot to learn.
1. Even if there are only a few people directly working on it at first, start a new public Slack channel dedicated solely to the project. This centralizes conversations and makes it easier to find history and share with others.
2. Communicate with other teams early if the work could span beyond your team. As these queries occur on a shared `issues-pull-requests` cluster, eventually the Issues team began work that overlapped with ours. If we'd started that conversation earlier we could have more quickly identified opportunities to collaborate.
3. Be careful with your feature flags. When we were troubleshooting the dual writing bugs, we opened a pull request to implement a new pathway while also removing the old way. On the surface that seemed like an efficient way to do it, but what we didn't consider was that the two ways were controlled by different feature flags. So there was a gap between when the old one was turned off and the new one was fully ramped up. This was a contributing factor to our record mismatches.
   
Additionally, this project generated some open questions for future development.
1. Currently, the queries section of a service's scorecard only passes if there are no killed/slow queries. Even one causes it to fail. Is there a way that we can make this more granular to better estimate impact and priority?
2. Finding and verifying database records was difficult and left us with low confidence. Is there a better way to identify which source to use, how, and when?
3. Transitions can be tricky and we encountered several setbacks. How can we improve this process to allow for better troubleshooting?

---
This was a major effort and included collaboration from many people and teams. Thank you to @jankoszewski, @another-mattr, @blakewilliams, @arthurschreiber, @hasan-dot, @ekroon, @christianlang, @derekprior, @elenatanasoiu, `@danhodos`, `@rufo`, and `@dzader`! (_am I missing anyone?_)

[^1]: If you want to see the full journey, you can follow along [here](https://github.com/github/mysql-database-usage/issues/1473#issuecomment-2539459465).
[^2]: https://github.com/github/pull-requests/issues/15675#issuecomment-2640038704
[^3]: https://github.com/github/github/pull/348838
[^4]: https://github.com/github/github/pull/361425
