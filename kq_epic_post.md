## The Fellowship of the Query: The Journey to a Greener Scorecard

#### TL;DR

The Pull Requests team resolved 109 killed/slow queries to achieve a green scorecard.

---
### The Quest
When SQL queries take more than 2 seconds they are considered "slow" and if they take more than 10 seconds they are "killed". Killed queries mean pages fail to load and database resources are wasted. Our [scorecard](https://catalog.githubapp.com/services/github/pull_requests/scorecards/mysql-database-usage), sets a stringent expectation of "_no killed queries on [primary/replica] database hosts generated by this service_". Supporting the full Pull Requests experience and all that it entails means a lot of queries across web and API calls. With availability as a priority, we began an epic to fix problematic queries and get our scorecard to green.

### Gather your equipment
The first step in any valiant quest is to study your map and pack your bag. For this epic, we relied on several powerful tools.
1. We often learn about problematic queries from automatically generated issues in the [mysql-database-usage repo](https://github.com/github/mysql-database-usage/issues?q=is%3Aissue%20state%3Aopen%20github%2Fpull_requests%20). This data allowed us to track, focus, and prioritize.
2. The mighty `EXPLAIN` was invaluable for testing possible performance wins on [staging clones]([https://thehub.github.com/epd/engineering/dev-practicals/mysql/running-explain-against-production/](https://thehub.github.com/epd/engineering/dev-practicals/mysql/running-explain-against-production/)) and measuring actual output on [a prod console](https://thehub.github.com/epd/engineering/dev-practicals/mysql/running-explain-against-production/).
3. We ran multiple [Scientist](https://github.com/github/scientist) experiments allowing us to iterate quickly and validate our changes while ensuring parity.

So just work your way from the top of the backlog, right? We soon found there were a few snags to this approach.
1. **The scorecard isn't static**. An existing query might be improved by an indirect change, or a new problematic query could arise from updated code or processes. With moving targets we needed to keep a closer eye on which queries were impacting us the most.
2. **Query identifiers change**. Each query is attached to a `fingerprint` and `mysql_digest` as unique identifiers. This group queries by their structure and origin,  and entry point. Sometimes a query's identifier changes. For example, in November 2024 several of our affected queries appeared to have been solved, but at the same time new issues were being created with identical query structures and entry points but with new fingerprints, [as explained here](https://github.com/github/mysql-database-scorecard/issues/57). Similarly, in July 2025 a change was made to identify queries by their `mysql_digest` instead of `fingerprint` resulting again in issues closing and new ones opening. This meant that we had to be vigilant about cross-referencing old query issues to new ones.
5. **Automations aren't perfect**. Killed query issues are regularly updated via automation with current performance data, which we used to determine impact and set priorities. Like any system, it's susceptible to bugs meaning that our data was delayed or inaccurate. It also can't tell when one query is a slight variation from another but fundamentally the same.

### Plan your adventure
With those challenges in mind, we gathered our fellowship and began to strategize. There were a couple of approaches we exploredw with the most common trait being a non-performant join on the `issues` table. Because pull requests continue to track their state with an issue under the hood, querying a pull request by its state required us to also query the `issues` table. This resulted in queries that looked something like this:
```
SELECT
    COUNT (*)
  FROM
    `pull_requests`
    , `issues`
  WHERE
    `pull_requests`.`repository_id` = ?
    AND (
      `pull_requests`.`user_hidden` = ?
      OR `pull_requests`.`user_id` = ?
    )
    AND `issues`.`state` = ?
    AND `issues`.`pull_request_id` = `pull_requests`.`id`
    AND `issues`.`repository_id` = `pull_requests`.`repository_id`
```
The basic question here is "Why can't a pull request have its own state?". The answer seemed like a clear win. Put a `state` column on `pull_requests` and remove the expensive join on issues `state`. Easy peasy.

### Face the challengers
Spoiler alert: It was not easy peasy.

#### Obstacle 1: The column that could not be named
An issue can be one of two states - open or closed. The same is true for pull requests at the database level. However, at the model level, a pull request can have three states: open, closed, or merged, as determined by the [PullRequest#state method](https://github.com/github/github/blob/a18286f131f4dc80dcbab605938125fcd9ac509d/packages/pull_requests/app/models/pull_request.rb#L650-L654). The existence of this method blocked us from simply adding a `pull_requests.state` field. We briefly explored some workarounds for this naming conflict, however, the best solution ended up being the simplest one; choose a different column name. With `pull_requests.status` in place, we could continue on our way.

#### Obstacle 2: We've had one, yes, but what about second join 
While the majority of the queries had a single join to issues for the state, there were some that were a bit more greedy. These came from GraphQL calls looking for a list of pull requests by label, and they required multiple joins to form a query that looked something like this:

```
SELECT
    `pull_requests`.`id`
  FROM
    `pull_requests`
    , `issues`
    , `issues_labels`
    , `labels`
  WHERE
    `labels`.`lowercase_name` IN (
      . . .
    )
    AND `pull_requests`.`repository_id` = ?
    AND (
      `pull_requests`.`user_hidden` = ?
      OR `pull_requests`.`user_id` = ?
    )
    AND `issues`.`state` = ?
    AND `issues`.`pull_request_id` = `pull_requests`.`id`
    AND `issues`.`repository_id` = `pull_requests`.`repository_id`
    AND `issues_labels`.`issue_id` = `issues`.`id`
    AND `labels`.`id` = `issues_labels`.`label_id`
    AND `labels`.`repository_id` = `issues`.`repository_id`
  ORDER BY
    `pull_requests`.`id` DESC
```

Simply swapping `issues.state` for `pull_request.status` wasn't enough for these queries. Thus we explored other optimizations, including breaking out subqueries, forcing or ignoring an index, and reorganizing the query structure, none of which showed improvements[^labels]. We even considered storing label ids in a new column on `issues` to eliminate the extra joins, but that presented its own big lift that was far outside the scope of this epic.

So, like the dwarves in the mines of Moria, we dug deeper to determine _who_ was hitting these queries. Over a period of 30 days, we found that between the two queries, all calls were coming from only 4 users within 4 repos. More interestingly, all 4 users were GitHub apps mimicking merge queue functionality. One in particular was routinely causing around 10k events every 20 minutes. With this new information, our minds turned towards rate limiting. Unfortunately, rate limits are set based on individual app installations and cannot be applied by query fingerprint, so they were too blunt to deploy here.

Not all slow and killed queries are equal and triaging is important. As a very small number of users were impacted, and because the lift would have been considerably high, we decided not to pursue this query at this time lest we awaken the Balrog.

#### Obstacle 3: The road goes ever on
To populate our new `pull_requests.status`column, we needed to backfill with the data from `issues.state`. The `pull_requests` table is _big_ so we knew we were in for a hefty process. What we didn't expect is that the transition would take 604 hours of run time across a period of 33 days, including dry runs. Within that, we had to maintain vigilance as we encountered unexplained failures that would interrupt the process, requiring us to manually restart. Finally, when we thought our toil had borne its reward, we were faced with some confusing data.

While the logging indicated everything had run as expected, Kusto, datadot, and analytics hosts suggested the backfill had missed some pull requests[^backfill]. This was potentially an enormous setback. To find a reliable source of truth, we reached out to several teams and finally learned that what we needed was a new full snapshot of the table. With that done, we were able to determine that the backfill was mostly successful and the remaining group of pull requests missing `status` data were from the period after dual writing was introduced.

#### Obstacle 4: But they were all of them deceived, for another bug was made
With more confidence in the backfilling, we began fixing our dual writing bug; ([a saga in itself](https://github.com/github/pull-requests/issues/15850)). Our initial implementation set a pull request's `status` at the same time as an `issue` state via an existing ActiveRecord callback on the issue[^callback]. The unexpected flaw was that an issue is created just before its associated pull request. Our first thought was to move this into its own callback on the pull request, but we decided to give orchestrations a try after some feedback from the Issues team[^orch]. But we still encountered new mismatched records as there now a race condition between two orchestration processes and further iterations on this path caused deadlocks. Ultimately, the solution involved a callback on the issue that was localized to update pull request `status` whenever an issue's `state` changed.

And yet, the wraiths were still at our heels. Due to the multiple iterations of dual-writing bug fixing, we now had out-of-sync pull request records, and we still had cleanup from the run of the initial transition to contend with. Instead of re-running the full transition, we took a more targeted approach by creating a transition that allowed us to iterate over targeted CSVs of the remaining records that were either mismatched or never backfilled.

#### Obstacle 5: One index to rule them all
One of most impactful changes for non-performant queries is [adding an index](https://thehub.github.com/epd/engineering/dev-practicals/mysql/optimizing-indexes/) and we attempted that several times during this project. Indexes aren't a one-size-fits-all solution, depend on the order of fields in the query, and can be expensive to add so we decided to start small and work incrementally. Initial attempts did yield results. In [this query](https://github.com/github/pull-requests/issues/15920#issuecomment-2810238155) used in the PullRequestSynchronization job, a new index reduced the cost by >99%! In tandem, the Issues team also [added indexes](https://github.com/github/github/pull/372015) that improved our shared queries. After analyzing and comparing the remaining killed/slow queries for our service, we saw the most success with a [covering index](https://github.com/github/github/pull/383003) including the most commonly filtered fields. This had the potential to [reduce query times by an average of 75%](https://github.com/github/pull-requests/issues/17975). There are some trade-offs for maintaining an index of this size but we determined that it was the best solution to address the most number of queries at this time, especially since GraphQL was a major entry point with highly variable queries. This change also gives us the opportunity to replace existing index iterations using the same fields.

#### Obstacles 6: There and back again (and again and again)
In one of our many collaborations with the Databases team, we discovered another great tool for our bag of holding, especially when it comes to pagination - [Common Table Expressions](https://dev.mysql.com/doc/refman/en/with.html). We were able to utilize this to successfully mitigate queries on pull request reviews[^reviews]. In this case, there was a non-trivial amount of killed queries coming almost entirely from two large repos where a bot was adding as many as 100k reviews on a single pull request. After testing subqueries and unions without gains, applying a CTE looked like a clear winner by reducing the overall time by ~85% when `explain`ing. We started to doubt ourselves when the Scientist experiment didn't yield the results we expected until we remembered how localized these queries were. By using a feature flag to test specifically on the problematic repos we got the results we were after.

[REVIEWS GRAPH]

### Revel in your triumph
Though the quest was arduous and many shadows befell us, at last we reaped the harvest of victory. We were able to mitigate 109 of the 115 queries we investigated during this epic.

[GRAPHS]

### Reflect on your journey

This project spanned many months and there was a lot to learn.
1. Even if there are only a few people directly working on it at first, start a new public Slack channel dedicated solely to the project. This centralizes conversations and makes it easier to find history and share with others.
2. Communicate with other teams early if the work could span beyond your team. As these queries occur on a shared `issues-pull-requests` cluster, eventually the Issues team began work that overlapped with ours. If we'd started that conversation earlier we could have more quickly identified opportunities to collaborate.
3. Be careful with your feature flags. When we were troubleshooting the dual writing bugs, we opened a pull request to implement a new pathway while also removing the old way. On the surface that seemed like an efficient way to do it, but what we didn't consider was that the two ways were controlled by different feature flags. So there was a gap between when the old one was turned off and the new one was fully ramped up. This was a contributing factor to our record mismatches.
4. When `EXPLAIN`ing queries, don't solely rely on the `cost`. It is somewhat arbitrary and doesn't always give the best information whereas 'actual time` is a more solid comparison.

Additionally, this project generated some open questions for future development.
1. The queries section of a service's scorecard only passes if there are no killed/slow queries. Even one causes it to fail. Is there a way that we can make this more granular to better estimate impact and priority?[^scorecard]
2. When attempting to verify the success of our transitions, we struggled to gain confidence as each data source returned different results. Is there a better way to identify which source to use, how, and when?
3. Transitions and migrations can be tricky and we encountered several setbacks, including throttling due to service contention. How can we improve these processes to allow for better operation and troubleshooting?

---
This was a major effort and included collaboration from many people and teams. Big thank yous to @jankoszewski, @another-mattr, @jamisonhyatt, @arthurschreiber, @blakewilliams, @hasan-dot, @ekroon, @christianlang, @derekprior, @elenatanasoiu, @codeminator, `@danhodos`, `@rufo`, and `@dzader`!

[^labels]: The full story is outlined [here](https://github.com/github/mysql-database-usage/issues/1473#issuecomment-2539459465).
[^backfill]: https://github.com/github/pull-requests/issues/15675#issuecomment-2640038704
[^callback]: https://github.com/github/github/pull/348838
[^orch]: This pattern is generally preferred over callbacks to prevent long-running transactions and lock contention; https://github.com/github/github/pull/361425
[^reviews]: https://github.com/github/mysql-database-usage/issues/2301
[^retention]: Due to data retention periods, it's difficult to calculate impact from when we began last year. https://app.datadoghq.com/notebook/12827498/prs-slow-killed-queries
[^scorecard]: There is [a discussion](https://docs.google.com/document/d/1BS1ZvCJ5k7vp73XlKj_Rh5XOxd7eYkpxpPW2AZIZWDM) open about this.
